# 4.2
由定义得:意未来状态的条件分布独立于过去的状态，只依赖于现在的状态  
所以得证

---

# 4.8
(a)
$R_i$ 是第 $i$ 个记录值。需要证明 $R_{i+1}$ 只依赖于 $R_i$
令 $R_1=X_{N_1}$ ，其中 $N_1$ 是第一个记录的时刻
$R_{i+1}$ 是第一个大于 $R_i$ 的 $X_n$
由于 $X_n$ 是独立同分布的，$R_{i+1}$ 的值只取决于 $R_i$ 和之后独立的 $X_n$ 的值，即：
$\{R_i,i\ge1\}$ 是 $Markov$ 链

转移概率计算：
$P\{R_{i+1}=s|R_i=r\}=\frac{a_s}{P\{X>r\}}$ ，$s>r$
$P\{X>r\}=\sum_{j>r}a_j$
因此，转移概率为：
$P_{rs}=\begin{cases} \frac{a_s}{\sum_{j>r}a_j} &s>r \\ 0 &其它 \end{cases}$

(b)
$\{T_i,i\ge1\}$ 的 $Markov$ 性：
$T_i$ 是第 $i$ 个记录与第 $i+1$ 个记录之间的时间间隔
由于记录的发生依赖于记录值 $R_i$ ，而 $T_i$ 本身不直接决定 $T_{i+1}$ ，因此 $\{T_i\}$ 不是 $Markov$ 链

$\{(R_i,T_i),i\ge1\}$ 的 $Markov$ 性：
由于 $X_n$ 是独立的，$(R_{i+1},T_{i+1})$ 的分布只依赖于 $R_i=r$ ，而不依赖于之前的 $(R_j,T_j)，(j<i)$ 。因此，$\{(R_i,T_i),i\ge1\}$ 是 $Markov$ 链

转移概率计算：
给定 $(R_i,T_i)=(r,t)$ ，$(R_{i+1},T_{i+1})=(s,k)$
$R_{i+1}=s$ ，$T_{i+1}=k$ 是几何分布：需要 $k-1$ 个 $X_n\le s$ ，第 $k$ 个 $X_n>s$
因此：
$P\{T_{i+1}=k|R_{i+1}=s\}=P\{X\le s\}^{k-1}P\{X>s\}$
$P\{(R_{i+1},T_{i+1})=(s,k)|(R_i,T_i)=(r,t)\}=\frac{a_s}{P\{X>r\}}\cdot P\{X\le s\}^{k-1}P\{X>s\}$

(c)
$S_n=\sum_{i=1}^{n}T_i$ 是第 $n$ 个记录发生的时刻
由于 $X_i$ 是连续的，$S_{n+1}=S_n+T_{n+1}$
$T_{n+1}$ 依赖于 $R_n$ ，但 $R_n=X_{S_n}$ 。由于 $X_i$ 是独立且连续的，$T_{n+1}$ 的分布只依赖于 $R_n=X_{S_n}$ ，而 $X_{S_n}$ 的值由 $S_n$ 决定
因此，$\{S_n\}$ 是 $Markov$ 链

转移概率计算：
给定 $S_n=s$ ，$S_{n+1}=s+k$ 的概率：
$P\{T_{n+1}=k|S_n=s\}=P\{X_{s+1},\cdots,X_{s+k-1}\le X_s,X_{s+k}>X_s|X_s=x\}$
由于 $X_i$ 独立于 $X_s$ ，且 $X_i$ 是连续的：
$P\{T_{n+1}=k|X_s=x\}=P\{X\le x\}^{k-1}P\{X>x\}$
$P\{T_{n+1}=k\}=\frac{1}{k(k+1)}$
$P\{S_{n+1}=s+k|S_n=s\}=P\{T_{n+1}=k\}=\frac{1}{k(k+1)}$ ，$k\ge1$

---

# 4.18
## (a)

$$
P_{0j} = \begin{cases}
\frac{e^{-\lambda t}(\lambda t)^j}{j!}, & 0 \leq j < N \\
1 - \sum_{k=0}^{N-1}\frac{e^{-\lambda t}(\lambda t)^k}{k!}, & j = N
\end{cases}
$$

当 $i \geq 1$ 时：

$$
P_{ij} = \begin{cases}
\frac{e^{-\lambda t}(\lambda t)^{j-i+1}}{(j-i+1)!}, & i-1 \leq j < N \\
1 - \sum_{k=i-1}^{N-1}\frac{e^{-\lambda t}(\lambda t)^{k-i+1}}{(k-i+1)!}, & j = N
\end{cases}
$$

## (b)

1. 不可约性：
   任意状态 $i$ 可以通过有限步转移到任意状态 $j$，因为对于 $j > i$：可以通过一段时间内来足够多的工作达到；而对于 $j < i$：可以通过每天处理一个工作且无新工作到达来达到

2. 非周期性：

   $$
   \begin{align*}
   P_{00}^{(2)} &> P_{01}P_{10} > 0 \\
   P_{00}^{(3)} &> P_{02}P_{21}P_{10} > 0
   \end{align*}
   $$

   因此状态 0 的周期 $d_0 = 1$. 注意到这是不可约的，所有所有状态的周期 $d=1$

3. 正常返性：
   由于状态空间有限且链不可约非周期，所有状态都是正常返的。

综上所述，该 Markov 链是遍历的。

## (c)

平稳分布 $\{\pi_j\}$ 应满足以下方程组：

$$
\begin{cases}
\pi_j = \sum_{i=0}^N \pi_i P_{ij}, & 0 \leq j \leq N \\
\sum_{j=0}^N \pi_j = 1
\end{cases}
$$

---

# 4.23
$$
\begin{align*}
&\mathbb{P}\{\text{他赢得下次赌局} | \text{当前资产是}\,i,\,\text{他迟早达到}\,N\} \\
&= \frac{\mathbb{P}(\text{当前资产}i,\text{迟早到}N | \text{赢得下局})\mathbb{P}(\text{赢得下局})}{\mathbb{P}(\text{当前资产}i,\text{迟早到}N)} \\
&= \frac{\mathbb{P}(\text{当前资产}i+1, \text{迟早到}N)p}{\mathbb{P}(\text{当前资产}i,\text{迟早到}N)} \\
&= \frac{f_{i+1}p}{f_i}
\end{align*}
$$

其中 $f_i$ 表示从 $i$ 开始财富迟早到 $N$ 的概率（参见例 4.4A）：

$$
f_i = \begin{cases}
\frac{1-(q/p)^i}{1-(q/p)^N} & \text{若}\,p \neq 1/2 \\
\frac{i}{N} & \text{若}\,p = 1/2
\end{cases}
$$

因此有：

$$
\mathbb P\{\text{他赢得下次赌局} | \text{当前资产是}\,i,\,\text{他迟早达到}\,N\} = \begin{cases}
p\frac{1-(q/p)^{i+1}}{1-(q/p)^i} & \text{若}\,p \neq 1/2 \\
\frac{i+1}{2i} & \text{若}\,p = 1/2
\end{cases}
$$
